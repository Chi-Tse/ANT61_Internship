{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adapted from Thomas Simonini's implementation\n",
    "#https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Q%20learning/FrozenLake/Q%20Learning%20with%20FrozenLake.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_map(size: int, p=0.5) -> list:\n",
    "    \"\"\"Generates a random valid map (one that has a path from start to goal)\n",
    "    :param size: size of each side of the grid\n",
    "    :param p: probability that a tile is a hole\n",
    "    \"\"\"\n",
    "    valid = False\n",
    "\n",
    "    # DFS to check that it's a valid path.\n",
    "    def is_valid(res, start: tuple):\n",
    "        frontier, discovered = [], set()\n",
    "        frontier.append(start)\n",
    "        while frontier:\n",
    "            r, c = frontier.pop()\n",
    "            if not (r, c) in discovered:\n",
    "                discovered.add((r, c))\n",
    "                directions = [(1, 0), (0, 1), (-1, 0), (0, -1)]\n",
    "                for x, y in directions:\n",
    "                    r_new = r + x\n",
    "                    c_new = c + y\n",
    "                    if r_new < 0 or r_new >= size or c_new < 0 or c_new >= size:\n",
    "                        continue\n",
    "                    if res[r_new][c_new] == \"G\":\n",
    "                        return True\n",
    "                    if res[r_new][c_new] != \"H\":\n",
    "                        frontier.append((r_new, c_new))\n",
    "        return False\n",
    "\n",
    "    while not valid:\n",
    "        p = min(1, p)\n",
    "        res = np.random.choice([\"F\", \"H\"], (size, size), p=[p, 1 - p])\n",
    "        # Generate random start/goal locations\n",
    "        start = (np.random.randint(size), np.random.randint(size))\n",
    "        goal = (np.random.randint(size), np.random.randint(size))\n",
    "        res[start] = \"S\"\n",
    "        res[goal] = \"G\"\n",
    "        valid = is_valid(res, start)\n",
    "    return [\"\".join(x) for x in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = 'FrozenLake8x8-v1'\n",
    "#map = 'FrozenLake-v1'\n",
    "b_slip = False\n",
    "random_map = False\n",
    "random_map_size = 6\n",
    "\n",
    "if random_map:\n",
    "    env = gym.make(id='FrozenLake8x8-v1', is_slippery=b_slip, desc = generate_random_map(size=random_map_size))\n",
    "else:\n",
    "    env = gym.make(id=map, is_slippery=b_slip)\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "qtable = np.zeros((state_size, action_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# default values for parameter experiments\n",
    "def train_model(total_episodes: int=20000, learning_rate: float=0.6, max_steps: int=200, gamma: float=0.6, \n",
    "                epsilon: float=1, max_epsilon: float=1, min_epsilon: float=0.0001, decay_rate: float=0.00005) -> list:\n",
    "    ep_reward = []\n",
    "    rewards_1000 = []\n",
    "    for episode in range(total_episodes):\n",
    "        # Reset the environment\n",
    "        global env\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # 3. Choose an action a in the current world state (s)\n",
    "            ## First we randomize a number\n",
    "            exp_exp_tradeoff = random.uniform(0, 1)\n",
    "            \n",
    "            ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n",
    "            if exp_exp_tradeoff > epsilon:\n",
    "                action = np.argmax(qtable[state,:])\n",
    "\n",
    "            # Else doing a random choice --> exploration\n",
    "            else:\n",
    "                action = env.action_space.sample()\n",
    "\n",
    "            # Take the action (a) and observe the outcome state(s') and reward (r)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
    "            # qtable[new_state,:] : all the actions we can take from new state\n",
    "            qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = new_state\n",
    "            if episode%5000==0:\n",
    "                env.render()\n",
    "                for event in pygame.event.get():\n",
    "                    pass\n",
    "            # Finish episode if agent reaches reward or hole\n",
    "            if done == True: \n",
    "                break\n",
    "            \n",
    "        # Reduce epsilon (because we need less and less exploration)\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n",
    "        ep_reward.append(total_rewards)\n",
    "\n",
    "        # Generate new map if randomise option is set\n",
    "        env.close()\n",
    "        if random_map == True:\n",
    "            env = gym.make(id = \"FrozenLake8x8-v1\", is_slippery=b_slip, desc = generate_random_map(size=random_map_size))\n",
    "    \n",
    "    rewards_1000 = np.add.reduceat(ep_reward, np.arange(0, len(ep_reward), 1000))\n",
    "    #print(rewards_1000[-1])\n",
    "    #print(epsilon)\n",
    "    #write_files(qtable, rewards_1000, total_episodes, learning_rate, gamma, min_epsilon, decay_rate)\n",
    "    return [qtable, rewards_1000]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files(qtable: np.ndarray, rewards_1000, total_episodes: int, learning_rate: float, gamma: float, \n",
    "                min_epsilon: float, decay_rate: float):\n",
    "\n",
    "    file_name = (str(total_episodes) +'_'+ str(learning_rate) +'_'+ str(gamma) +'_'+\n",
    "                 str(min_epsilon) +'_'+ str(decay_rate))\n",
    "    # path = Path(map)\n",
    "    # path.mkdir(exist_ok=True)\n",
    "    # with open (map + '\\\\' + file_name+'.txt', 'w') as f:\n",
    "    #     f.write('Total reward in final 1000 episodes: ' + str(rewards_1000[-1]) + '\\n' +\n",
    "    #             'Total episodes: ' + str(total_episodes) + '\\n' +\n",
    "    #             'Learning rate: ' + str(learning_rate) + '\\n' +\n",
    "    #             'Gamma: ' + str(gamma) + '\\n' +\n",
    "    #             'Min epsilon: ' + str(min_epsilon) + '\\n' +\n",
    "    #             'Decay rate: ' + str(decay_rate) + '\\n \\n' +\n",
    "    #             str(qtable))\n",
    "\n",
    "    # plt.plot(np.arange(0, total_episodes/1000), rewards_1000)\n",
    "    # plt.savefig(map + '\\\\'+file_name +'.png')\n",
    "    # plt.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ETCH\\Desktop\\Github\\ANT61 Internship\\Week 2\\Frozen_Lake.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ETCH/Desktop/Github/ANT61%20Internship/Week%202/Frozen_Lake.ipynb#ch0000005?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_model\u001b[39m(qtable: np\u001b[39m.\u001b[39mndarray, max_steps: \u001b[39mint\u001b[39m\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ETCH/Desktop/Github/ANT61%20Internship/Week%202/Frozen_Lake.ipynb#ch0000005?line=1'>2</a>\u001b[0m     env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ETCH/Desktop/Github/ANT61%20Internship/Week%202/Frozen_Lake.ipynb#ch0000005?line=3'>4</a>\u001b[0m     total_reward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def test_model(qtable: np.ndarray, max_steps: int=200):\n",
    "    env.reset()\n",
    "\n",
    "    total_reward = 0\n",
    "    total_episodes = 100\n",
    "    for episode in range(total_episodes):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Take the action (index) that have the maximum expected future reward given that state\n",
    "            action = np.argmax(qtable[state,:])\n",
    "            \n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            if episode%20==0:\n",
    "                env.render()\n",
    "                for event in pygame.event.get():\n",
    "                    pass\n",
    "            if done:\n",
    "                # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
    "\n",
    "                # We print the number of step it took.\n",
    "                print(\"Number of steps\", step)\n",
    "                print(\"Reward:\", reward)\n",
    "                total_reward += reward\n",
    "                break\n",
    "            state = new_state\n",
    "    print('Total rewards:', total_reward,'/',total_episodes)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning parameters\n",
    "#slippery 8x8\n",
    "#gamma = 0.7344827586 0.8172413793103449 0.7344827586206897\n",
    "#learning_rate = 0.1\n",
    "#non-slip 4x4 \n",
    "# learning_rate = 0.5413793103448277\n",
    "# gamma = 0.23793103448275865 0.34827586206896555\n",
    "#slip 4x4\n",
    "# gamma = 0.8724137931034484\n",
    "\n",
    "interval = 10\n",
    "interval2 = 10\n",
    "parameter_range = np.linspace(0.1,0.9,interval,endpoint=True)\n",
    "parameter_range2 = np.linspace(0.1,0.9,interval2,endpoint=True)\n",
    "rewards = []\n",
    "#rewards_sum = [0]*interval\n",
    "\n",
    "for i in range(1):\n",
    "    for parameter in parameter_range:\n",
    "        for parameter2 in parameter_range2:\n",
    "            # reset qtable\n",
    "            qtable = np.zeros((state_size, action_size))\n",
    "            qtable, rewards_1000 = train_model(total_episodes=20000, max_steps=100,\n",
    "                                                learning_rate=parameter, gamma=parameter2, decay_rate=0.0001)\n",
    "            rewards.append(rewards_1000[-1])\n",
    "rs_rewards = np.reshape(rewards,(-1,interval,interval2))\n",
    "rs_rewards_mean = np.mean(rs_rewards, axis=0)\n",
    "\n",
    "now = datetime.now().strftime('%Y%m%d-%H%M')   \n",
    "pd.DataFrame(rs_rewards_mean).to_csv(now + '.csv')\n",
    "\n",
    "#print('max reward:',max(rewards_mean), 'index:', rewards_mean.index(max(rewards_mean)))\n",
    "#print(parameter_range[rewards_mean.index(max(rewards_mean))])\n",
    "#plt.plot(parameter_range, rewards_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuned parameters\n",
    "if not random_map:\n",
    "    if env.spec.id == 'FrozenLake8x8-v1':\n",
    "        if b_slip:\n",
    "            # 8x8 slippery settings #####################################################################\n",
    "            total_episodes = 120000        # Total episodes\n",
    "            learning_rate = 0.01           # Learning rate\n",
    "            max_steps = 200                # Max steps per episode, capped at 100 for 4x4, 200 for 8x8\n",
    "            gamma = 0.999                  # Discounting rate\n",
    "            # Exploration parameters\n",
    "            epsilon = 1.0                 # Exploration rate\n",
    "            max_epsilon = 1.0             # Exploration probability at start\n",
    "            min_epsilon = 0.0001          # Minimum exploration probability \n",
    "            decay_rate = 0.00005          # Exponential decay rate for exploration prob\n",
    "        else:\n",
    "            # 8x8 non-slippery settings  #####################################################################\n",
    "            total_episodes = 60000        # Total episodes\n",
    "            learning_rate = 0.75           # Learning rate\n",
    "            max_steps = 200                # Max steps per episode, capped at 100 for 4x4, 200 for 8x8\n",
    "            gamma = 0.9                  # Discounting rate\n",
    "            # Exploration parameters\n",
    "            epsilon = 1.0                 # Exploration rate\n",
    "            max_epsilon = 1.0             # Exploration probability at start\n",
    "            min_epsilon = 0.0001            # Minimum exploration probability \n",
    "            decay_rate = 0.00005             # Exponential decay rate for exploration prob\n",
    "\n",
    "    elif env.spec.id == 'FrozenLake-v1':\n",
    "        if b_slip:\n",
    "            # 4x4 slippery settings  #####################################################################\n",
    "            total_episodes = 120000        # Total episodes\n",
    "            learning_rate = 0.1 #0.15517\n",
    "            # Learning rate\n",
    "            max_steps = 200                # Max steps per episode, capped at 100 for 4x4, 200 for 8x8\n",
    "            gamma = 0.9                  # Discounting rate\n",
    "            # Exploration parameters\n",
    "            epsilon = 1.0                 # Exploration rate\n",
    "            max_epsilon = 1.0             # Exploration probability at start\n",
    "            min_epsilon = 0.0001            # Minimum exploration probability \n",
    "            decay_rate = 0.00005             # Exponential decay rate for exploration prob\n",
    "        else:\n",
    "            # 4x4 non-slippery settings  #####################################################################\n",
    "            total_episodes = 60000        # Total episodes\n",
    "            learning_rate = 0.5413793103448277           # Learning rate\n",
    "            max_steps = 200                # Max steps per episode, capped at 100 for 4x4, 200 for 8x8\n",
    "            gamma = 0.23793103448275865                  # Discounting rate\n",
    "            # Exploration parameters\n",
    "            epsilon = 1.0                 # Exploration rate\n",
    "            max_epsilon = 1.0             # Exploration probability at start\n",
    "            min_epsilon = 0.0001            # Minimum exploration probability \n",
    "            decay_rate = 0.00005             # Exponential decay rate for exploration prob\n",
    "else: # random map\n",
    "    if b_slip:\n",
    "        # slippery settings  #####################################################################\n",
    "        total_episodes = 60000        # Total episodes\n",
    "        learning_rate = 0.36667           # Learning rate\n",
    "        max_steps = 200                # Max steps per episode, capped at 100 for 4x4, 200 for 8x8\n",
    "        gamma = 0.81111                  # Discounting rate\n",
    "        # Exploration parameters\n",
    "        epsilon = 1.0                 # Exploration rate\n",
    "        max_epsilon = 1.0             # Exploration probability at start\n",
    "        min_epsilon = 0.0001            # Minimum exploration probability \n",
    "        decay_rate = 0.00005             # Exponential decay rate for exploration prob\n",
    "    else:\n",
    "        # non-slippery settings  #####################################################################\n",
    "        total_episodes = 60000        # Total episodes\n",
    "        learning_rate = 0.36667\n",
    "           # Learning rate\n",
    "        max_steps = 200                # Max steps per episode, capped at 100 for 4x4, 200 for 8x8\n",
    "        gamma = 0.81111                  # Discounting rate\n",
    "        # Exploration parameters\n",
    "        epsilon = 1.0                 # Exploration rate\n",
    "        max_epsilon = 1.0             # Exploration probability at start\n",
    "        min_epsilon = 0.0001            # Minimum exploration probability \n",
    "        decay_rate = 0.00005             # Exponential decay rate for exploration prob\n",
    "\n",
    "qtable = np.zeros((state_size, action_size))\n",
    "qtable, rewards_1000 = train_model(total_episodes, learning_rate, max_steps, gamma, \n",
    "                                    epsilon, max_epsilon, min_epsilon, decay_rate)\n",
    "plt.plot(np.arange(0, total_episodes/1000), rewards_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************\n",
      "EPISODE  0\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  10\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  11\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  12\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  13\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  14\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  15\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  16\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  17\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  18\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  19\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  20\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  21\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  22\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  23\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  24\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  25\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  26\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  27\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  28\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  29\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  30\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  31\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  32\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  33\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  34\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  35\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  36\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  37\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  38\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  39\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  40\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  41\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  42\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  43\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  44\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  45\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  46\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  47\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  48\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  49\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  50\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  51\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  52\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  53\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  54\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  55\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  56\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  57\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  58\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  59\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  60\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  61\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  62\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  63\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  64\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  65\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  66\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  67\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  68\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  69\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  70\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  71\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  72\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  73\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  74\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  75\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  76\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  77\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  78\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  79\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  80\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  81\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  82\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  83\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  84\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  85\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  86\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  87\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  88\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  89\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  90\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  91\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  92\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  93\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  94\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  95\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  96\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  97\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  98\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "****************************************************\n",
      "EPISODE  99\n",
      "Number of steps 13\n",
      "Reward: 1.0\n",
      "Total rewards: 100.0 / 100\n"
     ]
    }
   ],
   "source": [
    "#slippery 8x8 qtable\n",
    "#test_qtable = np.loadtxt('C:\\\\Users\\\\ETCH\\\\Desktop\\\\Github\\\\ANT61 Internship\\Week 2\\\\FrozenLake8x8-v1_slippery\\\\120000_0.01_0.999_0.0001_5e-05.csv', delimiter =\",\")\n",
    "#non-slippery 8x8 qtable\n",
    "test_qtable = np.loadtxt('C:\\\\Users\\\\ETCH\\\\Desktop\\\\Github\\\\ANT61 Internship\\Week 2\\\\FrozenLake8x8-v1\\\\60000_0.75_0.9_0.0001_5e-05.csv', delimiter =\",\")\n",
    "test_model(qtable=test_qtable)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a92ebb1a32b68af07ba4ae21d89671d5ef172113d2414e62053b015791ee503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
