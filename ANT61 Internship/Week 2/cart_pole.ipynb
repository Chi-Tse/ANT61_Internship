{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Action space and state space explanation\n",
    "    # | Num | Action                 |\n",
    "    # |-----|------------------------|\n",
    "    # | 0   | Push cart to the left  |\n",
    "    # | 1   | Push cart to the right |\n",
    "    \n",
    "    # Min/Max values before episode ends\n",
    "    # | Num | Observation           | Min                  | Max                |\n",
    "    # |-----|-----------------------|----------------------|--------------------|\n",
    "    # | 0   | Cart Position         | -2.4                 | 2.4                |\n",
    "    # | 1   | Cart Velocity         | -Inf                 | Inf                |\n",
    "    # | 2   | Pole Angle            | ~ -0.2095 rad (-12°) | ~ 0.2095 rad (12°) |\n",
    "    # | 3   | Pole Angular Velocity | -Inf                 | Inf                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "action_size = env.action_space.n\n",
    "# Observation space halved to fit within allowable values before episode ends\n",
    "state_space = np.array([env.observation_space.low, env.observation_space.high])/2\n",
    "state_size = [10]*len(state_space[0])\n",
    "discrete_interval = (state_space[1]-state_space[0])/state_size\n",
    "q_table = np.zeros((state_size+[action_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(total_episodes: int=20000, learning_rate: float=0.6, max_steps: int=200, gamma: float=0.6, \n",
    "                epsilon: float=1, max_epsilon: float=1, min_epsilon: float=0.0001, decay_rate: float=0.00005) -> list:\n",
    "    ep_rewards = []\n",
    "    rewards_1000 = []\n",
    "\n",
    "    # for episode in range(total_episodes):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for i in range(100000):\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        #print(action, new_state)\n",
    "        total_reward += reward\n",
    "        env.render(mode='human')\n",
    "        if done == True: \n",
    "            ep_rewards.append(total_reward)\n",
    "            #break\n",
    "    print(ep_rewards)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ETCH\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:163: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "train_model()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9a92ebb1a32b68af07ba4ae21d89671d5ef172113d2414e62053b015791ee503"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
